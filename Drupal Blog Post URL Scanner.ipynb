{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "5\n",
      "entering page0\n",
      "currently scanning http://www.burkemuseum.org/blog/topic/spider-myths\n",
      "retrieving all blog post data...\n",
      "entering page1\n",
      "currently scanning http://www.burkemuseum.org/blog/topic/spider-myths?page=1\n",
      "retrieving all blog post data...\n",
      "entering page2\n",
      "currently scanning http://www.burkemuseum.org/blog/topic/spider-myths?page=2\n",
      "retrieving all blog post data...\n",
      "entering page3\n",
      "currently scanning http://www.burkemuseum.org/blog/topic/spider-myths?page=3\n",
      "retrieving all blog post data...\n",
      "entering page4\n",
      "currently scanning http://www.burkemuseum.org/blog/topic/spider-myths?page=4\n",
      "retrieving all blog post data...\n",
      "entering page5\n",
      "currently scanning http://www.burkemuseum.org/blog/topic/spider-myths?page=5\n",
      "retrieving all blog post data...\n",
      "YAS\n",
      " Spider Myths resources \n",
      "done\n"
     ]
    }
   ],
   "source": [
    "## Written by Jason Chen, Former Burke Web Assistant 2018\n",
    "## This program scan clusters of blog, e.g. \"spider myths\" in \"blog\"\n",
    "## and find if there is a specific hyperlink within the article of the cluter\n",
    "## e.g. you might ask, \"is there a 'www.abc.com' within the 'spider' blogs\"\n",
    "## Only feasible for current Burke Website template as of 1/31/18\n",
    "## Feel free to reach out for questions: scierc@uw.edu\n",
    "\n",
    "# METHODS #\n",
    "# URL = url of the topic page (FOR TOTAL PAGE NUMBER = get ul class = \"pager\", len() - 2)\n",
    "# find all the \"views-row views-row\" for all blog content! \n",
    "# for each of them, get \"views-field views-field-view-node\", and the href attr\n",
    "# URL2 = above\n",
    "# simply use find! if result > 0, then output \"views-field views-field-title\"\n",
    "# METHODS #\n",
    "\n",
    "import urllib2\n",
    "import urllib\n",
    "from docx import Document\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "### ENTER THE URL YOU WANT TO FIND HERE ###\n",
    "find = 'http://www.ama-assn.org/amednews/2002/08/05/hlsa0805.htm'\n",
    "### ENTER THE URL YOU WANT TO FIND HERE ###\n",
    "\n",
    "#initiation 1 = get the page number!\n",
    "### ENTER THE URL OF THE BLOG CLUTERS TWICE HERE ###\n",
    "url = \"http://www.burkemuseum.org/blog/topic/spider-myths\"\n",
    "ori_url = \"http://www.burkemuseum.org/blog/topic/spider-myths\"\n",
    "### ENTER THE URL OF THE BLOG CLUTERS TWICE HERE ###\n",
    "\n",
    "html = urllib2.urlopen(url).read() \n",
    "soup = BeautifulSoup(html,\"html.parser\")\n",
    "ul = soup.find('ul', {'class' : \"pager\"})\n",
    "page = ul.findAll('li', {'class':'pager-item'})\n",
    "\n",
    "\n",
    "count = 0\n",
    "# print 'here'\n",
    "# print len(page)\n",
    "while count <= len(page):\n",
    "    print 'entering page ' + str(count)\n",
    "    if count == 0:\n",
    "        url = url\n",
    "        count = count + 1\n",
    "    else:\n",
    "        url = url + '?page=' + str(count)\n",
    "        count = count + 1        \n",
    "    print 'currently scanning ' + url    \n",
    "    #open the current url    \n",
    "    html = urllib2.urlopen(url).read() \n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "    rows = soup.findAll(\"div\", {\"class\" : lambda L: L and L.startswith('views-row views-row-')})\n",
    "    #go in and parse through each column element, if there \n",
    "    rows = iter(rows)\n",
    "    next(rows)\n",
    "    print 'retrieving all blog post data...'\n",
    "    for i in rows:\n",
    "        title = i.find('div', {'class' : \"views-field views-field-title\"})\n",
    "        read_more = i.find('div', {'class' : \"views-field views-field-view-node\"})\n",
    "        #print 'retrieving each Read More link...'\n",
    "        href = read_more.find('a')['href']\n",
    "        target_url = 'http://www.burkemuseum.org' + href\n",
    "        target_html = urllib2.urlopen(target_url).read() \n",
    "        soup = BeautifulSoup(target_html,\"html.parser\")\n",
    "        target_all_href = soup.findAll('a')\n",
    "        #print 'scanning each blog post within Read More...'\n",
    "        for i in target_all_href:\n",
    "            if i.get('href') == find:\n",
    "                print 'Yas Target Found'\n",
    "                print ' The blog title is:'\n",
    "                print title.text\n",
    "    url = ori_url\n",
    "print 'done'   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
